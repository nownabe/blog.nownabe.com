---
title: "機械学習勉強会 20170706"
category: Log
tags: 機械学習勉強会, 統計学入門
created_at: 2017-07-19 23:41:11 +0900
updated_at: 2017-08-04 19:02:49 +0900
published: true
number: 1056
---

<div class="asin">
<div class="asin-image"><a href="https://www.amazon.co.jp/exec/obidos/ASIN/4130420658/nownabe0c-22/" rel="nofollow noopener" target="_blank"><img src="http://images-jp.amazon.com/images/P/4130420658.09._SL160_.jpg" alt="統計学入門 (基礎統計学Ⅰ)" title="統計学入門 (基礎統計学Ⅰ)"></a></div>
<div class="asin-detail">
<p><a href="https://www.amazon.co.jp/exec/obidos/ASIN/4130420658/nownabe0c-22/" rel="nofollow noopener" target="_blank">統計学入門 (基礎統計学Ⅰ)</a></p>
<ul>
<li>東京大学教養学部統計学教室</li>
<li>東京大学出版会</li>
</ul>
</div>

<p></p>
</div>

今は[統計学入門](https://www.amazon.co.jp/exec/obidos/ASIN/4130420658/nownabe0c-22/)。
週一で1時間程度各自でもくもくして、章末問題で同期を取るという方法でやってみている。

# 今回の内容
## 第3章 2次元のデータ
### 3.4 直線および平面のあてはめ
#### 3.4.1 直線のあてはめ
* 2変数$x, y$の間に、$x$が$y$を左右ないしは決定する関係があるとき$x$を**独立変数 (independent variable)**、$y$を**従属変数 (dependent variable)**という
* **最小二乗法 (method of least squares)**
    * 2次元データを$y=bx+a$の直線として$a, b$を決定する方法
    * **二乗和 (sum of squares)**を最小にする$a, b$の値を求める
        * $L=\sum\\{y_i-(bx_i+a)\\}^2$
    * 得られた1次式を**回帰方程式 (regression equation)** あるいは **回帰直線 (regression line)**と呼ぶ
    * **偏回帰係数 (partial regression coefficient)** **傾き (slope)**
        * $b=\frac{\sum{x_iy_i}-n\overline{x}\overline{y}}{\sum{x_i^2}-n\overline{x}^2}$
    * **$y$切片 ($y$-intercept)**
        * $a=\overline{y}-b\overline{x}$
* **決定係数 (coefficient of determination)**
    * 相関係数の二乗 $r^2$
    * $r^2=1$ならデータは完全に直線上にあり、$y$は$x$から完全に決定される

#### 3.4.2 平面の当てはめ
* **重回帰 (multiple regression)**
    * 独立変数が2つ以上ある場合の回帰
    * $y=b_1x_1+b_2x_2+\dots+b_px_p+a$
    * 直線へ当てはめは単回帰 (simple regression)という

#### 3.4.3 多項式回帰
* **多項式回帰 (polynomial regression)**
    * 2次式や3次式でのあてはめ

# 所感
* ブログで数式が展開されてなかった :joy:

# 次回

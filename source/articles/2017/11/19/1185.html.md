---
title: "Pythonではじめる機械学習  3回目"
category: Log
tags: 機械学習勉強会, 統計学入門
created_at: 2017-11-15 19:12:04 +0900
updated_at: 2017-11-19 17:58:59 +0900
published: true
number: 1185
---


[nownab.log | Pythonではじめる機械学習 2回目](https://blog.nownabe.com/2017/11/08/1174.html)

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="//rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=nownabe0c-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=as_ss_li_til&asins=4873117984&linkId=05656b0761603e4e9f88423f102e42c6"></iframe>

週一でやっていて、毎週読む範囲を決めて資料にまとめて発表するという感じでやっている。

また、勉強会で書いたコードや疑問点などをまとめるためにGitHubのレポジトリを活用している。
[Wondershake/machine-learning-study: 機械学習勉強会](https://github.com/Wondershake/machine-learning-study)

この記事は資料作りの下書き的扱い。

# 3章 教師なし学習と前処理
## 3.1 教師なし学習の種類
* 教師なし変換 (Unsupervised transformations)
    * もとのデータを変換して、人間やアルゴリズムにわかりやすいデータを作る
    * 次元削減、トピック抽出
* クラスタリングアルゴリズム (Clustering algorithms)
    * データをグループに分ける

## 3.2 教師なし学習の難しさ
* アルゴリズムの評価が難しい
    * 出力がどうあるべきかわからない
* データを理解するために用いられることが多い
* 教師あり学習の前処理としても使われる

## 3.3 前処理とスケール変換
### 3.3.1 さまざまな前処理
* StandardScaler
    * それぞれの特徴量が平均0、分散1となるように変換する
* RobustScaler
    * 平均値と分散のかわりに中央地と四分位数を用いる
    * 外れ値を無視する
* MinMaxScaler
    * データが0から1の間になるように変換する
* Normalizer
    * 特徴量ベクトルのユークリッド長が1になるように変換する
    * データポイントを超球面に投射する

### 3.3.2 データ変換の適用
### 3.3.3 訓練データとテストデータを同じように変換する
* 訓練データにfitしたものを使ってテストデータを変換する
* テストデータでfitしてはダメ

### 3.3.4 教師あり学習における前処理の効果

## 3.4 次元削減、特徴量抽出、多様体学習
### 3.4.1 主成分分析 (PCA)
* データセットの特徴量を回転する
    * まず最も分散が大きい方向をみつけ、その方向の軸を第1成分とする
    * 第1成分と直行する軸のうち最も情報を持つ軸を探し第2成分とする
    * これを繰り返す
* それぞれの軸を主成分という
* もとの特徴量と同じだけ主成分が存在する

### 3.4.1.1 cancerデータセットのPCAによる可視化
* PCAは高次元データセットの可視化で用いられることがある
* 主成分の解釈が簡単ではないことが多い

### 3.4.1.2 固有顔による特徴量抽出
* PCAを特徴量抽出に使う
    * より機械学習アルゴリズムに適したデータに変換する
* Labeled Faces in the Wild
* 主成分の一部で元のデータをある程度再現できる
    * すべての主成分を使えば完全に再現できる

### 3.4.2 非負値行列因子分解 (NMF)
* 特徴量を抽出することを目的とする教師なし学習手法
* 係数と成分が非負
    * 特徴量が非負のデータにしか使えない
* 複数の独立した発生源からのデータを重ね合わせたデータに有効
* NMFはPCAよりも理解しやすい成分に分解する

#### 3.4.2.1 NMFの合成データへの適用
* NMFの成分に順番はない
* 乱数で初期化するので毎回結果が変わる

#### 3.4.2.2 NMFの顔画像への適用
* 再構成やエンコードは苦手
* データ中から興味深いパターンを見つけるのに用いられる
* 音声データ、遺伝子発見、テキストデータにうまく機能する
* 合成データをうまく分離できる

### 3.4.3 t-SNEを用いた多様体学習
* 多様体学習アルゴリズムは可視化によく用いられる
    * 特に有用なのがt-SNE
* 訓練データにしか使えない
    * 探索的なデータ解析に有用
    * 教師あり学習の前処理には用いない
* デフォルトのパラメータで大抵うまくいく

## 3.5 クラスタリング
* クラスタリングはデータセットをクラスタと呼ばれるグループに分割するタスク

### 3.5.1 k-meansクラスタリング
* k-meansは最も単純で最も広く用いられているクラスタリングアルゴリズム
* クラスタ重心を見つけていく
    * ランダムでクラスタセンタを選択
    * 個々のデータポインタを最も近いクラスタセンタのクラスタに割り当てる
    * それぞれのクラスタの重心をクラスタセンタに設定し直す
    * 繰り返し、収束したら終了

#### 3.5.1.1 k-meansがうまくいかない場合
* k-meanのクラスタは凸形状であるという強い仮定がある
    * 複雑な形のクラスタにはうまく対応できない

#### 3.4.1.2 ベクトル量子化、もしくは成分分解としてのk-means
* PCAやNMFなどの成分分解手法との類似性がある
    * 個々のデータポイントをクラスタセンタという単一の成分で表現していると考えることができる
* ベクトル量子化 (vector quantization)
    * k-meansを単一成分で個々のデータポイントを表現する成分分解手法としてみる考え方
* k-meansによるベクトル量子化では入力次元よりはるかに多くのクラスタを使うことができるのが特徴
* k-meansの欠点の1つは乱数で初期化されること


# 所感
* k-meansのベクトル量子化の項はそういう使い方もあるのか！という感じだった

```math
```

---
title: "Pythonではじめる機械学習  4回目"
category: Log
tags: 機械学習勉強会, 統計学入門
created_at: 2017-11-27 16:55:33 +0900
updated_at: 2017-12-08 16:37:48 +0900
published: true
number: 1199
---

[nownab.log | Pythonではじめる機械学習 3回目](https://blog.nownabe.com/2017/11/19/1185.html)

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="//rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=nownabe0c-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=as_ss_li_til&asins=4873117984&linkId=05656b0761603e4e9f88423f102e42c6"></iframe>

週一でやっていて、毎週読む範囲を決めて資料にまとめて発表するという感じでやっている。

また、勉強会で書いたコードや疑問点などをまとめるためにGitHubのレポジトリを活用している。
[Wondershake/machine-learning-study: 機械学習勉強会](https://github.com/Wondershake/machine-learning-study)

この記事は資料作りの下書き的扱い。

# 3章 教師なし学習と前処理
## 3.5 クラスタリング
### 3.5.2 凝集型クラスタリング
* **凝集型クラスタリング (agglomerative clustering)**
    * 個々のデータポイントを1つのクラスタとして開始し、類似するクラスタを併合していくようなクラスタリングのアルゴリズム
* 様々な類似度がある
    * ward / average / complete
    * だいたいward使っとけばOK
* 新しいデータに対する予測はできない

#### 3.5.2.1 階層型クラスタリングとデンドログラム
* 凝集型クラスタリングは**階層型クラスタリング (hierarchical clustering)**を行う
* **デンドログラム (dendrogram)**で可視化できる

### 3.5.3 DBSCAN
* **DBSCAN (dencity-based spatial clustering of applications with noise)**
    * 密度に基づくノイズあり空間クラスタリング
* クラスタ数を自動決定する
* k-meansより遅い
* 比較的大きなデータセットにも適用できる
* どのクラスタにも属さないデータポイントをノイズとする
    * 外れ値検出に使える
* 近さの閾値とクラスタを構成する最小のデータ数がパラメータ

### 3.5.4 クラスタリングアルゴリズムの比較と評価
#### 3.5.4.1 正解データを用いたクラスタリングの評価
* 重要な指標
    * **調整ランド指数 (adjusted rand index: ARI)**
    * **正規化相互情報量 (normalized mutual information: NMI)**
* Accuracyは評価に適さない

#### 3.5.4.2 正解データを用いないクラスタリングの評価
* クラスタリングタスクでは正解データがない場合が多い
* **シルエット係数 (silhouette coefficient)**
    * 正解データを必要としないクラスタリングの評価指標
    * 実際にはうまく評価できない
* 頑健性を用いた評価指標がある

#### 3.5.4.3 顔画像データセットを用いたアルゴリズムの比較
### 3.5.5 クラスタリング手法のまとめ
* クラスタリングは評価も含めて定性的
* データの探索的な解析に最も役立つ
* k-means
    * クラスタセンタでクラスタの特徴を表すことが可能
* 凝集型クラスタリング
    * 階層的なクラスタリングが可能
* DBSCAN
    * クラスタ数を自動決定
    * ノイズ検出が可能

# 4章 データの表現と特徴量エンジニアリング
* **連続値特徴量 (continuous feature)**だけでなく、**カテゴリ特徴量 (categorical feature)**も扱う必要がある
* **特徴量エンジニアリング (feature engineering)**
    * 最良のデータ表現を模索すること

## 4.1 カテゴリ変数
* 性別や学歴などはそのままではロジスティック回帰などのアルゴリズムで学習できない

### 4.1.1 ワンホットエンコーディング (ダミー変数)
* **ワンホットエンコーディング (one-hot-encoding)**
    * カテゴリ変数を1つ以上の、0か1を取る新しい特徴量で置き換える方法
    * 性別の場合は次のような2つの特徴量として表す
        * 男の場合は1、それ以外は0
        * 女の場合は1、それ以外は0
* pandasを使うと簡単

#### 4.1.1.1 文字列で表されているカテゴリデータのチェック
* 表記ゆれなどを前処理する必要がある
* Seriesクラスの`value_counts`メソッドでチェックできる
* `get_dummies`関数でワンホットエンコーディングできる

### 4.1.2 数値でエンコードされているカテゴリ
* そのままではpandasにカテゴリ変数として解釈されない
    * scikit-learnのOneHotEncoderを使う
    * DataFrameの列名を文字列に変換する

## 4.2 ビニング、離散化、線形モデル、決定木
* **ビニング (binning)**
    * **離散化 (discretization)**とも
    * 線形モデルを連続データに対してより強力にする
    * データのレンジを複数のビンとして分割し、そのデータがどのビンに入るかをワンホットエンコーディングで表す
    * それぞれのビン内ではすべて同じ予測をする
    * データが大きくて高次元な場合で、線形モデルを使って非線形なモデルを構築したいときに有効

## 4.3 交互作用と多項式
* **交互作用特徴量 (interaction feature)**
    * `x_product = np.hstack([x_binned, x * x_binned])`
* **多項式特徴量 (polynomial feature)**
    * `x ** 2`や`x ** 3`のような特徴量
    * `PolynomialFeatures`で実現可能

## 4.4 単変量非線形変換
* `x ** 2`や`x ** 3`のような特徴量は線形回帰モデルで有用
* **単変量非線形変換**
    * log, exp, sinなどを使ってデータを変換する
    * 元々非線形なデータを線形モデルで扱えるようにする
    * ニューラルネットワークモデル等にも有効

# 所感
* まとめるの遅くなった :disappointed: 

```math
```

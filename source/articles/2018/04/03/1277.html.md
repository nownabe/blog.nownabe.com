---
title: "言語処理のための機械学習入門 第7回"
category: Log
tags: ml, 機械学習勉強会, 言語処理のための機械学習入門, nlp
created_at: 2018-04-03 14:09:15 +0900
updated_at: 2018-04-03 16:13:52 +0900
published: true
number: 1277
---

[nownab.log | 言語処理のための機械学習入門 第4回](https://blog.nownabe.com/2018/03/14/1271.html)

# 概要
機械学習勉強会で輪読してる[言語処理のための機械学習入門](http://amzn.to/2BFQSee)の学習メモです。
勉強会用の資料があるのでこちらでは資料に載らなかったメモ等を。

# 範囲
* 2. 文書および単語の数学的表現
    * 2.1 タイプ, トークン
    * 2.2 nグラム
    * 2.3 文書, 文のベクトル表現
    * 2.4 文書に対する前処理とデータスパースネス問題
    * 2.5 単語のベクトル表現
    * 2.6 文書や単語の確率分布による表現
    * 2.7 この章のまとめ
    * 章末問題

# 資料

# 2 文書および単語の数学的表現

## 2.1 タイプ, トークン

自分でなにかするときはもちろん意識してるけど会話するときは「単語」としか言ってなかったので, 会話の中で単語トークンと単語タイプを使い分けるのやったほうがよさそう.

## 2.2 nグラム

### 2.2.1 単語nグラム

### 2.2.2 文字nグラム

## 2.3 文書, 文のベクトル表現

### 2.3.1 文書のベクトル表現

頻度を素性値に使うものを頻度ベクトル, 出現したかどうかのみ気にするのを二値ベクトル. これもあんま使い分けしてなかったので気をつけよう.

### 2.3.2 文のベクトル表現

## 2.4 文書に対する前処理とデータスパースネス問題

### 2.4.1 文書に対する前処理

### 2.4.2 日本語の前処理

### 2.4.3.データスパースネス問題

## 2.5 単語のベクトル表現

### 2.5.1 単語トークンの文脈ベクトル表現

### 2.5.2 単語タイプの文脈ベクトル表現

単純に足し合わせるだけなのかー. 正規化とか必要そう.

## 2.6 文書や単語の確率分布による表現

## 2.7 この章のまとめ

## 章末問題

```rb
# frozen_string_literal: true

def make_ngram(n, arr)
  print "#{n}-gram: "
  vector = Hash.new { |h, k| h[k] = 0 }
  arr.each_cons(n).each_with_object(vector) { |c, v| v[c.join] += 1 }.sort_by { |a| a[0] }
end

def print_ngram(ngram)
  puts "[" + ngram.map { |e| e.join(":") }.join(", ") + "]"
end

puts "======== 1 ========"

word = "tattarrattat"

print_ngram(make_ngram(1, word.chars))
print_ngram(make_ngram(2, word.chars))
print_ngram(make_ngram(3, word.chars))

puts
puts "======== 2 ========"

sentence = "A cat sat on the mat."
stop_words = %w[a the on in of]

dic = { "sat" => "sit" }

doc = sentence.split(" ")
doc.map! { |w| w.tr(".", "") }
doc.map(&:downcase)
doc.map! { |w| dic.key?(w) ? dic[w] : w }
doc.delete_if { |w| stop_words.include?(w) }
print_ngram(make_ngram(1, doc))

puts
puts "======== 3 ========"

sentence = "I had a supercalifragilisticexpialidocious time with friends."
puts "[-2=had:1, -1=a:1, +1=time:1, +2=with:1]"
```

3番… :innocent:

```
======== 1 ========
1-gram: [a:4, r:2, t:6]
2-gram: [ar:1, at:3, ra:1, rr:1, ta:3, tt:2]
3-gram: [arr:1, att:2, rat:1, rra:1, tar:1, tat:2, tta:2]

======== 2 ========
1-gram: [A:1, cat:1, mat:1, sit:1]

======== 3 ========
[-2=had:1, -1=a:1, +1=time:1, +2=with:1]
```

# 所感

Word2Vecとかも詳しく勉強してみたいなー.

# 言語処理のための機械学習入門
<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="//rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=nownabe0c-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=as_ss_li_til&asins=4339027510&linkId=1c6291b86381f20d113796257356ef1b"></iframe>

# 機械学習勉強会について
社内で毎週開催している。本書は輪読形式でまわしているが、題材によって形式は柔軟に変えている。
毎週読む範囲を決めて資料にまとめて発表するという感じでやっている。

また、勉強会で書いたコードや疑問点などをまとめるためにGitHubのレポジトリを活用している。
[Wondershake/machine-learning-study: 機械学習勉強会](https://github.com/Wondershake/machine-learning-study)

```math
```

---
date: 2017-10-17T00:29:41+0900
lastmod: 2017-10-25T20:42:10+0900
tags: ["機械学習勉強会", "統計学入門"]
draft: false
isCJKLanguage: true

title: "統計学入門  第13章 回帰分析"
category: Log

created_at: 2017-10-17 00:29:41 +0900
updated_at: 2017-10-25 20:42:10 +0900
published: true
number: 1155
---

[nownab.log | 統計学入門 第12章 仮説検定](https://blog.nownabe.com/2017/10/25/1165.html)

<div class="asin">
<div class="asin-image"><a href="https://www.amazon.co.jp/exec/obidos/ASIN/4130420658/nownabe0c-22/" rel="nofollow noopener" target="_blank"><img src="http://images-jp.amazon.com/images/P/4130420658.09._SL160_.jpg" alt="統計学入門 (基礎統計学Ⅰ)" title="統計学入門 (基礎統計学Ⅰ)"></a></div>
<div class="asin-detail">
<p><a href="https://www.amazon.co.jp/exec/obidos/ASIN/4130420658/nownabe0c-22/" rel="nofollow noopener" target="_blank">統計学入門 (基礎統計学Ⅰ)</a></p>
<ul>
<li>東京大学教養学部統計学教室</li>
<li>東京大学出版会</li>
</ul>
</div>

<p></p>
</div>

機械学習勉強会として今は[統計学入門](https://www.amazon.co.jp/exec/obidos/ASIN/4130420658/nownabe0c-22/)をやっている。
週一でやっていて、今週から輪読形式で進めてみることになった。

また、勉強会で書いたコードや疑問点などをまとめるためにGitHubのレポジトリを活用している。
[Wondershake/ml-statistics-intro: 基礎統計学 I 統計学入門 (東京大学出版会)](https://github.com/Wondershake/ml-statistics-intro)

# 今回の内容
* **回帰分析 (regression analysis)**
    * 2変数$X, Y$のデータがあるとき**回帰方程式 (regression equation)**を求めること
    * $X$と$Y$の定量的な関係の構造(**モデル model**)を求めること
    * $Y$を$X$で**説明 (explain)**しようとする
    * $Y$を**従属変数、非説明変数、内生変数**などと呼ぶ
    * $X$を**独立変数、説明変数、外生変数**などと呼ぶ

## 13.1 回帰分析
* 回帰方程式
    * $y=\beta_1+\beta_2x$
    * **回帰関数 (regression function)**とも
    * **線形回帰 (linear regression)**: $y$が$x$の線形関数である場合
    * **非線形回帰 (non-linear regression)**: $y$が$x$の線形関数でない場合
    * 本書では線形回帰のみを扱う
* **母回帰方程式 (population regression equation)**
    * $Y_i=\beta_1+\beta_2X_i+\epsilon_i (i=1, 2, \dots, n)$
    * $\beta_1, \beta_2$: **母(偏)回帰係数 (population (partial) regression coefficient)**
    * $\epsilon_i$: **誤差項 (error term)、撹乱項 (disturbance term)**といい、次の条件を満たす
        * $E(\epsilon_i)=0$
        * $V(\epsilon_i)=\sigma^2$
        * $i\neq j$ならば$Cov(\epsilon_i,\epsilon_j)=E(\epsilon_i\epsilon_j)=0$

## 13.2 回帰係数の推定
### 13.2.1 最小二乗法による回帰係数の推定
* **最小二乗法 (method of least squares)**
    * なるべく誤差が小さくなるように回帰係数を推定する
    * 次の$S$を最小にする$\hat{\beta_1}, \hat{\beta_2}$を推定量とする
    * $S=\sum\epsilon^2=\sum\\{Y_i-(\beta_1+\beta_2X_i)\\}^2$
    * $\hat{\beta_1}, \hat{\beta_2}$を$\beta_1, \beta_2$の**最小二乗推定量 (least squares estimator)**という
* **正規方程式 (normal equation)**
    * $S$の一次の偏微分を0とおいたときの連立方程式
    * $n\beta_1+(\sum X_i)\beta_2=\sum Y_i$
    * $(\sum X_i)\beta_1+(\sum X_i^2)\beta_2=\sum X_iY_i$
* **標本(偏)回帰係数 (sample (partial) regression coefficient)**
    * $\hat{\beta_2}=\frac{\sum(X_i-\bar{X})(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2}$
    * $\hat{\beta_1}=\bar{Y}-\hat{\beta_2}\bar{X}$
* **標本回帰方程式 (sample regression equation)**又は**標本回帰直線 (sample regression line)**
    * $Y=\hat{\beta_1}+\hat{\beta_2}X$
* $\hat{Y_i}=\hat{\beta_1}+\hat{\beta_2}X_i$は$E(Y_i)$の推定量となり**回帰値 (regressed value)**とよばれる
* **回帰残差 (residual)**
    * $\hat{e_i}=Y_i-\hat{\beta_1}+\hat{\beta_2}X_i$
    * 以下の性質を満たす
        * $\sum\hat{e_i}=0$
        * $\sum\hat{e_i}X_i=0$
* **推定値の標準誤差 (standard error of estimates)**
    * 回帰値のあてはまりの良し悪しの目安
    * s.e.で表す
    * $s^2=\frac{\sum\hat{e_i}^2}{n-2}$ 誤差項$\epsilon_i$の分散$\sigma^2$の推定
    * $s$が推定値の標準誤差
* **標準化(偏)回帰係数 (standardized (partial) regression coefficient)**
    * $\frac{(X_i-\bar{X})}{s_x}, \frac{(Y_i-\bar{Y})}{s_y}$について回帰分析を行ったときの偏回帰係数
    * 切片の推定量は0、傾きの推定量は相関係数と一致する

### 13.2.2 回帰方程式の当てはまりと決定係数 $\eta^2$
* **決定係数** $\eta^2$
    * 回帰方程式の当てはまりのよさをはかる基準
    * $\eta^2=1-\frac{\sum(\hat{Y_i}-\bar{Y})^2}{\sum(Y_i-\bar{Y})^2}$
    * $\eta^2=(標本相関係数)^2$
* $\hat{\beta_1}, \hat{\beta_2}$は以下の性質があり**最良な推定量である**
    * $E(\hat{\beta_1})=\beta_1$
    * $E(\hat{\beta_2})=\beta_2$
    * それぞれ分散が最小
    * **最良線形不偏推定量 (BLUE, best linear unbiased estimator)**
* **ガウス・マルコフの定理 (Gauss-Markov's theorem)**
    * $\hat{\beta_1}, \hat{\beta_2}$が最良ですよという定理

## 13.3 偏回帰係数の統計的推測
* 回帰分析は母偏回帰係数についての検定も目的

### 13.3.1 偏回帰係数の標本分布
* 誤差項が3つの条件に加えて正規分布$N(0, \sigma^2)$に従うとする
* $\beta_2$は$N(\beta_2, \frac{\sigma^2}{\sum(X_i-\bar{X})^2}$に従う
    * $\sigma^2$は未知なので標準誤差を用いる
    * $s.e.(\hat{\beta_2})=\frac{s.e.}{\sqrt{\sum(X_i-\bar{X})}}$
    * 標準化した$\hat{\beta_2}$の$\sigma$を$s.e.$で置き換えた$t_2=\frac{\hat{\beta_2}-\beta_2}{s.e.(\hat{\beta_2})}$は自由度$n-2$のt分布に従う
* $\hat{\beta_1}$は$N(\beta_1, \frac{\sigma^2\sum X_i^2}{n\sum(X_i-\bar{X})^2})$に従う
    * $s.e.(\hat{\beta_1})=s.e.\cdot\sqrt{\displaystyle\frac{\sum X_i^2}{n\sum(X_i-\bar{X})^2}}$
    * $t_1=\frac{\hat{\beta_1}-\beta_1}{s.e.(\hat{\beta_1})}$

### 13.3.2 偏回帰係数の検定
* $\hat{\beta_2}$を検定する
    * 帰無仮説 $H_0:\beta_2=a$
    * 両側検定
        * 対立仮説 $H_1:\beta_2\neq a$
        * $|t_2|\geq t_{\frac{\alpha}{2}}(n-2)$のとき帰無仮説を棄却
    * 片側検定
        * 対立仮説 $H_1:\beta_2>a$
        * $|t_2|\geq t_{\alpha}(n-2)$のとき帰無仮説を棄却
* $H_0:\beta_2=0$という帰無仮説が採択されたときは$X$が$Y$を説明できないことになり回帰式が適切でないといえる
* $t_2$を**t値 (t-ratio)**ということが多い

## 13.4 重回帰分析
* **単純回帰分析 (simple regression analysis)** (**単回帰分析**)
    * 説明変数が1つの場合
* **重回帰分析 (multiple regression analysis)**
    * 複数の説明変数の場合
    * $Y_i=\beta_1+\beta_2X_{2i}+\beta_3X_{3i}+\cdots+\beta_kX_{ki}+\epsilon_i$
    * 単純回帰分析と同様に誤差項を最小にする最小二乗法が用いられる
    * 帰無仮説が複数の制約式から成る場合はF検定を用いる

# 練習問題
[第13章 回帰分析 · Issue #27 · Wondershake/ml-statistics-intro](https://github.com/Wondershake/ml-statistics-intro/issues/27)


# 所感
* 終わったーーーー！！！！
* 練習問題も全部解いたし、理解できてなかったところを理解したという実感もあり、達成感が半端ない

```math
```

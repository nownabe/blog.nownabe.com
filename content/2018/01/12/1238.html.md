---
date: 2018-01-12T17:08:41+0900
lastmod: 2018-01-12T17:08:41+0900
tags: ["Python機械学習プログラミング", "機械学習勉強会"]
draft: false
isCJKLanguage: true

title: "Python機械学習プログラミング  第2章"
category: Log

created_at: 2018-01-12 17:08:41 +0900
updated_at: 2018-01-12 17:08:41 +0900
number: 1238
---

# 概要
Python機械学習プログラミングの学習メモです。
勉強会用の資料があるのでこちらでは資料に載らなかったメモ等を。

# 資料
* [2.1-2.4](https://gitpitch.com/mmyoji/slides/python-ml-1st#/)
* [2.5-2.6](https://nownabe.github.io/slides/20180111-python-ml-ch02-1/index.html#/)

# メモ
## 2.1 人工ニューロン
## 2.2 パーセプトロンの学習アルゴリズムをPythonで実装する
## 2.3 Iris データセットでのパーセプトロンモデルのトレーニング
## 2.4 ADALINEと学習の収束

"ADALINE"の発音は"アダライン"でいいっぽい。

* https://www.youtube.com/watch?v=hc2Zj55j1zU
* https://www.youtube.com/watch?v=SRRcrl4WBnc

## 2.5 勾配降下法によるコスト関数の最小化
### 2.5.1 ADALINEをPythonで実装する
## 2.6 大規模な機械学習と確率的勾配降下法

何が"確率的"なのかわからなかったので調べてみた。

> そこで勾配 $\nabla L(w)$ の代わりに，その不偏推定量である確率的勾配(stochastic gradient) $\nabla l(w,z)$ を用いた勾配法が SGD である
>
> [確率的勾配降下法 - 数理計画用語集](http://www.msi.co.jp/nuopt/glossary/term_da265770bed70e5f0a764f3d20c0ce3d242e6467.html)

うーん、わからん。ランダムにサンプリングするから確率的？

# Python機械学習プログラミング
<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="//rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=nownabe0c-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=as_ss_li_til&asins=B01HGIPIAK&linkId=ef8aa25f336a01f62b00fce21e6f952a"></iframe>

# 機械学習勉強会について
社内で毎週開催している。本書は輪読形式でまわしているが、題材によって形式は柔軟に変えている。
毎週読む範囲を決めて資料にまとめて発表するという感じでやっている。

また、勉強会で書いたコードや疑問点などをまとめるためにGitHubのレポジトリを活用している。
[Wondershake/machine-learning-study: 機械学習勉強会](https://github.com/Wondershake/machine-learning-study)

```math
```

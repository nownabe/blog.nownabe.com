---
date: 2018-01-29T17:56:06+0900
lastmod: 2018-01-31T17:38:58+0900
tags: ["機械学習勉強会", "Python機械学習プログラミング"]
draft: false
isCJKLanguage: true

title: "Python機械学習プログラミング  第5章"
category: Log

created_at: 2018-01-29 17:56:06 +0900
updated_at: 2018-01-31 17:38:58 +0900
number: 1244
---

# 概要
Python機械学習プログラミングの学習メモです。
勉強会用の資料があるのでこちらでは資料に載らなかったメモ等を。

今回は、第5章 次元削減でデータを圧縮する

前回: [nownab.log | Python機械学習プログラミング 第4章](https://blog.nownabe.com/2018/01/24/1242.html)

# 資料
* [5.1]()
* [5.2]()
* [5.3](https://nownabe.github.io/slides/20180201-python-ml-ch05/index.html#/)

# 環境

本書の各バージョンは結構古いので、できるだけ新しいバージョンを使うようにしている。

* Python 3.6.4
* scikit-learn 0.19.1
* matplotlib 2.1.1

# メモ
## 5.1 主成分分析による教師なし次元削減

急に共分散行列とか固有値が出てきて、は？ってなったけど下の記事がだいぶわかりやすかった。

[固有ベクトル、主成分分析、共分散、エントロピー入門 | コンピュータサイエンス | POSTD](http://postd.cc/a-beginners-guide-to-eigenvectors-pca-covariance-and-entropy/)

### 5.1.1 共分散行列の固有対を求める

### 5.1.2 特徴変換

`np.newaxis`とはなんぞ。ということで調べてみた。`np.newaxis`自体はただの`None`。numpyのsliceに自動でshapeをよろしくやってくれる機能があって、`None`のかわりに`np.newaxis`って書くとかっこいいぜってことらしい。


```python
>>> a = np.array([1, 2, 3])
>>> b = np.array([4, 5, 6])
>>> a[:, np.newaxis]
array([[1],
       [2],
       [3]])

# Noneでも同じ
>>> a[:, None]
array([[1],
       [2],
       [3]])

# reshapeでもできる
>>> a.reshape(3, 1)
array([[1],
       [2],
       [3]])

# numpyじゃないと効かない
>>> c = [1, 2, 3]
>>> c[:, np.newaxis]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: list indices must be integers or slices, not tuple
```

### 5.1.3 scikit-learn の主成分分析

## 5.2 線形判別分析による教師ありデータ圧縮

ここでのLDAはLinear Discriminant Analysisで、Latent Dirichlet Allocationとは別物

全然関係ないけどなんでnotesにも同じこと書いてあるんだ…？誤植？

### 5.2.1 変動行列を計算する

### 5.2.2 新しい特徴部分空間の線形判別を選択する

### 5.2.3 新しい特徴空間にサンプルを射影する

### 5.2.4 scikit-learn による LDA

`sklearn.lda.LDA`はもうなくなってたので、`sklearn.discriminant_analysis.LinearDiscriminantAnalysis`を使った。
ディリクレさんのLDAが実装されたのと同じタイミングで名前変わったっぽいから混同しないようにかな。

## 5.3 カーネル主成分分析を使った非線形写像
### 5.3.1 カーネル関数とカーネルトリック
### 5.3.2 Pythonでカーネル主成分分析を実装する

[scipy.spatial.distance.pdist — SciPy v1.0.0 Reference Guide](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html)

seuclideanは標準化されたユークリッド距離

```python
>>> x = np.array([[1,2,3],[4,5,6],[7,8,9]])
>>> pdist(x)
array([ 5.19615242, 10.39230485,  5.19615242])
```

[scipy.spatial.distance.squareform — SciPy v1.0.0 Reference Guide](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.squareform.html)

```python
>>> sq_dists
array([ 5.19615242, 10.39230485,  5.19615242])
>>> squareform(sq_dists)
array([[ 0.        ,  5.19615242, 10.39230485],
       [ 5.19615242,  0.        ,  5.19615242],
       [10.39230485,  5.19615242,  0.        ]])
```

### 5.3.3 新しいデータ点を射影する

### 5.3.4 scikit-learn のカーネル主成分分析



# Python機械学習プログラミング
<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="//rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=nownabe0c-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=as_ss_li_til&asins=B01HGIPIAK&linkId=ef8aa25f336a01f62b00fce21e6f952a"></iframe>

# 機械学習勉強会について
社内で毎週開催している。本書は輪読形式でまわしているが、題材によって形式は柔軟に変えている。
毎週読む範囲を決めて資料にまとめて発表するという感じでやっている。

また、勉強会で書いたコードや疑問点などをまとめるためにGitHubのレポジトリを活用している。
[Wondershake/machine-learning-study: 機械学習勉強会](https://github.com/Wondershake/machine-learning-study)

```math
```

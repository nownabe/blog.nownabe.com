---
date: 2018-01-15T00:53:58+0900
lastmod: 2018-01-17T22:17:32+0900
tags: ["機械学習勉強会", "Python機械学習プログラミング"]
draft: false
isCJKLanguage: true

title: "Python機械学習プログラミング  第3章"
category: Log

created_at: 2018-01-15 00:53:58 +0900
updated_at: 2018-01-17 22:17:32 +0900
number: 1240
---

# 概要
Python機械学習プログラミングの学習メモです。
勉強会用の資料があるのでこちらでは資料に載らなかったメモ等を。

今回は、第3章 分類問題 - 機械学習ライブラリ scikit-learn の活用

前回: [nownab.log | Python機械学習プログラミング 第2章](https://blog.nownabe.com/2018/01/12/1238.html)

# 資料
* [3.1-3.3]()
* [3.4-3.7](https://nownabe.github.io/slides/20180118-python-ml-ch03/index.html)

# 環境

本書の各バージョンは結構古いので、できるだけ新しいバージョンを使うようにしている。

* Python 3.6.4
* scikit-learn 0.19.1
* matplotlib 2.1.1

# メモ
## 3.1 分類アルゴリズムの選択
## 3.2 scikit-learn活用へのファーストステップ
### 3.2.1 scikit-learnを使ったパーセプトロンのトレーニング

プログラムを何箇所か修正した。

cross_validationモジュールはscikit-learn 0.18から非推奨になのでmodel_selectionモジュールを使うようにした。

[Perceptron](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)の`n_iter`パラメータはscikit-learn 0.19から非推奨なので`max_iter`パラメータを使うようにした。

`plot_decision_regions`関数が本書のプログラムだとテストサンプルを囲う丸が表示されなかったので以下のように`edgecolors`を追加した。[^1]

[^1]: [matplotlib.pyplot.scatter — Matplotlib 2.1.1 documentation](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.scatter.html#matplotlib.pyplot.scatter)

```python
    if test_idx:
        X_test, y_test = X[test_idx, :], y[test_idx]
        plt.scatter(
            X_test[:, 0],
            X_test[:, 1],
            c="",
            edgecolors="black",
            alpha=1.0,
            linewidths=1,
            marker="o",
            s=55,
            label="test set"
        )
```

また、`plot_decision_regions`は結構使う予感がするので、トップディレクトリの`util.py`というファイルに定義した。各章でディレクトリを切っているので、使うときはこんな感じになる。[^2]

[^2]: もっとましなimportの仕方はないのか…。

```python
import sys, os
sys.path.append(os.pardir)
from util import plot_decision_regions
```

## 3.3 ロジスティック回帰を使ったクラスの確率のモデリング
### 3.3.1 ロジスティック回帰の直感的知識と条件付き確率

なんでlogit関数の方から説明してあるんだろうか…。

### 3.3.2 ロジスティック関数の重みの学習

尤度関数 $L(\boldsymbol{w})=P(\boldsymbol{y}|\boldsymbol{x; w})$ を使って最尤推定する。実際は負の対数尤度関数をコスト関数として勾配法でパラメータを求める。

### 3.3.3 scikit-learn によるロジスティック回帰モデルのトレーニング

`predict_proba`のところがそのままだと次のエラーが出る。

```
ValueError: Expected 2D array, got 1D array instead:
array=[0.70793846 1.50872803].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
```

次のように修正した。

```python
print(lr.predict_proba(X_test_std[0:1, :]))
```

### 3.3.4 正則化による化学週への対処

3.3.22式と3.3.24式が等価じゃない気がするんですが…。

正則化パラメータと重みをプロットするコードがそのままだと次のエラーが出る。

```
ValueError: Integers to negative integer powers are not allowed.
```

次の1行を追加した。

```python
c = np.int(c)
```

ロジスティック回帰の詳しい説明はこの本がオススメらしい。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="//rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=nownabe0c-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=as_ss_li_til&asins=4320018672&linkId=c4f3bad11834727e6686d620fde154cd"></iframe>

## 3.4 サポートベクトルマシンによる最大マージン分類
### 3.4.1 最大マージンを直感的に理解する
詳しくは

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="//rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=nownabe0c-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=as_ss_li_til&asins=4061529064&linkId=390de8411a89fa3b9fee1455d8c57b82"></iframe>

### 3.4.2 スラック変数を使った非線形分離可能なケースへの対処
### 3.4.3 scikit-learn での代替実装

## 3.5 カーネル SVM を使った非線形問題の求解
### 3.5.1 カーネルトリックを使って分離超平面を高次元空間で特定する

## 3.6 決定木学習
### 3.6.1 情報利得の最大化: できるだけ高い効果を得る

### 3.6.2 決定木の構築
### 3.6.3 ランダムフォレストを使って弱い学習アルゴリズムと強い学習アルゴリズムを結合する

なぜ復元抽出なんだろうか。一般的なブートストラップ法も復元抽出をするので、それを踏襲してるだけなのか。ブートストラップ法は非復元じゃだめなのか。

元の分布と似た別のデータが手に入る、ってとこが重要なのかな。統計的には人工的にデータを増やして母集団を近似するって意味があるっぽい。

[An Introduction to the Bootstrap - Bradley Efron, R.J. Tibshirani - Google ブックス](https://books.google.co.jp/books?id=gLlpIUxRntoC&printsec=frontcover&hl=ja&source=gbs_ge_summary_r&cad=0#v=onepage&q&f=false)

この辺に書いてあんのかな。。。

## 3.7 k近傍法: 怠惰学習アルゴリズム

# Python機械学習プログラミング
<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="//rcm-fe.amazon-adsystem.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=nownabe0c-22&o=9&p=8&l=as4&m=amazon&f=ifr&ref=as_ss_li_til&asins=B01HGIPIAK&linkId=ef8aa25f336a01f62b00fce21e6f952a"></iframe>

# 機械学習勉強会について
社内で毎週開催している。本書は輪読形式でまわしているが、題材によって形式は柔軟に変えている。
毎週読む範囲を決めて資料にまとめて発表するという感じでやっている。

また、勉強会で書いたコードや疑問点などをまとめるためにGitHubのレポジトリを活用している。
[Wondershake/machine-learning-study: 機械学習勉強会](https://github.com/Wondershake/machine-learning-study)

```math
```
